---
title: "Via Network Analysis"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
  html_notebook: default
---

# Fisrt steps
packages
```{r, include=FALSE}
library("qgraph")
library("bootnet")
library("dplyr")
library("NetworkComparisonTest")
library("mgm")
library("RColorBrewer")
library("viridis")
library(reshape2) #to melt correlation matrix
library(ggplot2)
library(reshape2) #to reshape objects in R
library(gplots)
library(igraph)
library(devtools)
library(FactoMineR)#install from github @kassambara/factoextra
library(xlsx)
library("FactoMineR") 
library("factoextra")
library("readxl")
library("latex2exp")
library('wTO')
library('EGAnet')
library('progress')
library(psych) 
library("writexl")
```




Creating the dataset to work in R.
```{r}
via_raw<-read.table("Network.dat", 
           header=TRUE, sep="\t")
```



Transforming columns 6 and 7 as.Date:
```{r, include=FALSE}
via_raw$DateOfBirth<-as.Date(via_raw$DateOfBirth,format="%m/%d/%Y")
via_raw$SurveyDate<-as.Date(via_raw$SurveyDate,format="%m/%d/%Y")
```
creating new subset with factors:
```{r}
via_factors<-via_raw[,1:31]
```

```{r}
#making a table of statistics
stats <- describe(via_factors)
stats$vars <- row.names(stats)
write_xlsx(stats,"stats.xlsx")
```



viewing all the histograms:
```{r}
# Reduzco a 5 dígitos el nombre para que sea más legible en nodos:
colnames <- c("Beaut","Brave","Love","Prude","TWork",     
"Creat","Curio","Fairn","Forgi","Grati", "Hones", "Hope", "Humor", "Perse", "Judgm", "Kindn", "Leade", "Learn", "Humil", "Persp", "SelfR", "SocIQ", "Spiri", "Zest")
```


```{r}
#Histograms
via_hist_df<-via_factors[,8:31]
layout(matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24), 4, 6, byrow = TRUE))
colnames <- colnames
for (i in 1:24) {
    hist(via_hist_df[,i], main=colnames[i], probability=TRUE, col="black", border="white")
}

```



## Regularized Partial correlation network:
Lets try to compute lasso regularized gaussian graphical model and Triangulated Maximally Filtered Graph. La idea subyacente del TMFG consiste en construir una triangulación que maximice una función de puntuación asociada a la cantidad de información retenida por el grafo. El TMFG utiliza como pesos cualquier medida de similitud arbitraria para ordenar los datos en una estructura de red significativa que puede utilizarse para la agrupación, la detección de comunidades y el modelado.

```{r}
set.seed(666)
via_GGM <- estimateNetwork(via_factors[,8:31], default="EBICglasso", tuning=0.4)

```
visualizing:
```{r fig.height=10}
pdf("img/network_glasso.pdf", width = 20, height = 15)
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="grey50",legend=FALSE, aspect=T, labels = colnames, theme = "TeamFortress", label.color = 'grey10', border.color = 'grey50' )
dev.off()
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="grey50",legend=FALSE, aspect=T, labels = colnames, theme = "TeamFortress", label.color = 'grey10', border.color = 'grey50')

```





We can create a object of adjacency Matrix:
```{r}
via_adjacencyMatrix <- via_GGM$graph
```

## Heat plot:

```{r}
heatmap.2(via_adjacencyMatrix, 
        main="Adjacency Matrix", 
        trace="none",
        col = viridis(24),
        scale="none")

```

# Centrality analysis
## EBIC GLASSO

Node strength
```{r}
centRes <- centrality(via_GGM)

# Node strength (degree):
centRes$OutDegree # Or InDegree, it's the same en networks sin dirección
```
```{r}
# Closeness:
centRes$Closeness
```

```{r}
# Betweenness:
centRes$Betweenness
```
ploting:
```{r}
centralityPlot(via_GGM, include = 'all')
```





```{r}
pdf("img/expectedInfluence.pdf", width = 10, height = 5)
centralityPlot(GGM =  via_GGM, include = 'ExpectedInfluence', scale = 'relative', orderBy='ExpectedInfluence')
dev.off()
centralityPlot(GGM =  via_GGM, include = 'ExpectedInfluence', scale = 'relative', orderBy='ExpectedInfluence')
```






# Community detection

M. Girvan and M. E. J. Newman in one of their researches, have highlighted the structure-property of the community using social networks and biological networks. According to them, network nodes are tightly connected in point clusters within communities and weakly connected between communities.

Why communities and not clustering (unsupervised learning)?
It can be argued that community detection is similar to clustering. Clustering is a machine learning technique in which similar data points are grouped into the same cluster based on their attributes. Although clustering can be applied to networks, it is a broader field of unsupervised machine learning that deals with multiple types of attributes. On the other hand, community detection is especially designed for network analysis, which relies on a single type of attribute: edges. Moreover, clustering algorithms tend to separate single outlying nodes from the communities to which they should belong. However, both clustering and community detection techniques can be applied to many network analysis problems and may have different pros and cons depending on the domain.
Community detection methods can be broadly classified into two types: agglomerative methods and divisive methods. In agglomerative methods, edges are added one by one to a graph containing only nodes. The edges are added from the strongest to the weakest. Divisive methods are the opposite of agglomerative methods. In them, edges are removed one by one from a complete graph.

## Igraph
We can create a net from adjacency matrix
```{r fig.height=10}
via_net<-graph_from_adjacency_matrix(via_adjacencyMatrix, mode="undirected")
```
We can try with 
```{r}
 
# Estimate networks, first a Gaussian Graphical Model, then an Information Filtering Network:
data = via_factors[,8:31]



set.seed(2018)
graphGGM<-plot(via_GGM)

g = as.igraph(graphGGM, attributes=TRUE)




```




## GALSSO



Community Strucure Via Short Random Walks
This function tries to find densely connected subgraphs, also called communities in a graph via random walks. The idea is that short random walks tend to stay in the same community.

```{r}
lc <- walktrap.community(g)
mem <- membership(lc)
com <- communities(lc)
pdf("img/walktrap.pdf", width = 10, height = 5)
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="black", legend=F, aspect=T, groups = com, title="Comunities in VIA (Community Strucure Via walktrap.community) GLASSO")
dev.off()
plot(via_GGM_plot)

```

```{r}
modularity(g,mem, weights = NULL)
```

Spinglass algorithm


```{r}
lc <- spinglass.community(g, weights = )
mem <- membership(lc)
com <- communities(lc)
pdf("img/spinglass.pdf", width = 10, height = 5)
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="black", legend=F, aspect=T, groups = com, title="Comunities in VIA (Community Strucure Via SPINGLASS) GLASSO")
dev.off()
plot(via_GGM_plot)

```
```{r}
modularity(g,mem, weights = abs(E(g)$weight))
```

# Probability that node i is in community with node j
The algorithm has several parts:
1. It takes a sample from the database.
2. It makes a network by Glasso partial correlations.
3. It calculates communities (Spinglass)
4. Extract communities
5. For each node i calculate the probability of being with each node i+1 ... i+n-1 (where n=24 is the number of total nodes in the case of VIA). Note that in each iteration the matrix will be dichotomous, of 0 and 1.
6. A matrix is declared with the results for the first iteration (it is a binary matrix).
7. Sum with the general matrix, which starts as a matrix of zeros and in each iteration adds the new result.
8. Once the whole loop is finished, the matrix is divided by the number of bootstrapping to be done.
9. Plot the results

```{r}
iter = 100 #número de iteraciones totales del bootstrapping
n_sample = 10000 #numero de observaciones por sampleo
megamatrix <- via_adjacencyMatrix * 0 #inicio una megamatrix con todas las probabilidades a 0
magamatrix_zero <- via_adjacencyMatrix * 0 # otra que me sirve para poner a cero las samples
# spinglass
for(i in 1:iter) {
# Tomando una muestra de la base de datos
df_sample = sample_n(via_factors, n_sample)
#Hace una red por correlaciones parciales Glasso
via_GGM_sample <- estimateNetwork(df_sample[,8:31], default="EBICglasso", tuning=0)
#Calcula comunidades (Spinglass)
pdf(paste("img/com/redes/red_bs", "_",i,".pdf"), width = 10, height = 5)
graphGGM_sample <- plot(via_GGM_sample)
dev.off()
g_sample = as.igraph(graphGGM_sample)
sg_sample <- walktrap.community(g_sample)
#Extraigo comunidades
com_sample <- communities(sg_sample)
#ploteo solo para tener en cuenta lo que va pasando
pdf(paste("img/com/communities/spinglass_bs", "_",i,".pdf"), width = 10, height = 5)
via_GGM_plot_sample<-plot(via_GGM_sample, layout="spring", vsize=6, cut=0, 
                          border.width=1.5, border.color="black", 
                          legend=F, aspect=T, 
                          groups = com_sample, 
                          title="Comunities in VIA (Community Strucure Via SPINGLASS) GLASSO")
dev.off()
megamatrix_sample <- magamatrix_zero
n_grupos <- length(com_sample)
for(nodo in 1:nrow(megamatrix)){
  for(grupo in 1:n_grupos){
    if (nodo %in% com_sample[[grupo]]){
      megamatrix_sample[nodo, com_sample[[grupo]]] <-1
    }
  }
  
}
megamatrix <- megamatrix + megamatrix_sample #sumo el resultado a la megamatrix
#por cada iteración del bootstraping
}



megamatrix <- megamatrix/(iter)

```

```{r}
pdf("img/comunities_probabilities.pdf", width = 20, height = 15)
graph_BS<-qgraph(megamatrix,  layout="spring", vsize=6, cut=0, border.width=1.5, legend=F, aspect=T, groups = com, title="Comunities probabilities bootstraping in VIA (Community Strucure Via SPINGLASS) GLASSO", labels = colnames, theme = "TeamFortress",  curveAll=0.5)
dev.off()
```
```{r}
pdf("img/expectedInfluence_bootstraping.pdf", width = 10, height = 5)
centralityPlot(GGM =  megamatrix, include = 'all', scale = 'relative')
dev.off()
centralityPlot(GGM =  megamatrix, include = 'all', scale = 'relative')
```


# Bridge Centrality
It takes communities into account, so we should decide which communities we choose and with which method (see below). Once the communities have been decided, it can be put as an argument in bridge centrality to study the centrality between communities of the nodes with the Jones package. For now I develop it according to the default calculation of the communities that appears in the package, which according to its own code "if communities not supplied, use spinglass default settings to detect" uses Spinglass.


```{r}
# hago las redes
require(qgraph)
n = nrow(via_factors)
network = via_GGM_plot
require("networktools")
bridge_centrality <- bridge(network)
bridge_centrality
```

```{r}
# ploteamos:
pdf("img/bridge_centrality_glasso.pdf")
plot(bridge_centrality, include=c("Bridge Strength", "Bridge Expected Influence (1-step)"), zscore=FALSE)
dev.off()
plot(bridge_centrality, include=c("Bridge Strength", "Bridge Expected Influence (1-step)"), zscore=FALSE)
```


We can select those that are in the 80% of Exp-influence:

```{r}
BEI1 <- bridge_centrality$`Bridge Expected Influence (1-step)`
communities <- bridge_centrality$communities
top_bridges <- names(BEI1[BEI1>quantile(BEI1, probs=0.80, na.rm=TRUE)])
top_bridges
```

We generate a new group in our communities who are the most bridge EI


```{r}
bridge_num_w1 <- which(names(BEI1) %in% top_bridges)
new_communities <- vector()
for(i in 1:length(BEI1)) {
  if(i %in% bridge_num_w1) {
    new_communities[i] <- "Bridge"
  } else {new_communities[i] <- communities[i]}
}
```

```{r}
pdf("img/communities_new_group.pdf")
grafo <-qgraph(network, layout="spring", 
       groups=new_communities, color=c("red","#eee361", "#6fb3e4", "#469b77", "#d16d6f", "#bbb2d4"), 
       legend.cex = 0.4)
dev.off()
qgraph(network, layout="spring", 
       groups=new_communities, color=c("red","#eee361", "#6fb3e4", "#469b77", "#d16d6f", "#bbb2d4"), 
       legend.cex = 0.4)
```



# Computing Predictability 
we would like to know: How accurate can the prediction be for a given node knowing all the others? Predictability is interesting for several reasons: 1. Even if a node has several edges but you can only predict 1% of its variance, then, we could come to the conclusion that we cannot assure that by influencing the nodes to which it is connected you can have good results.
2. It tells us to what extent the different parts of the network are self-determined or determined by other factors that are not included in the network.

Now you can get the predictability we have from using other machine learning methods. See the jupyter notebook. 
If you want the code of the jupyternotebook please contact gustavo+analytics@nirakara.com

```{r}
#### Gráfico 1. Práctica - Blink
predictions_error <- read_xlsx("predictions_error.xlsx")[,2:25]

predictions_error <- t(predictions_error)
colnames(predictions_error) <- c("RMSE","MAE", "R2","aR2")
predictions_error <- as.data.frame(predictions_error)
predictions_error[predictions_error<=0] <- 0
```

```{r}

pdf("img/predictability.pdf", width = 20, height = 15)
qgraph(via_GGM$graph, # weighted adjacency matrix as input
       cut=0,
       legend=FALSE, aspect=T,
       layout = 'spring', 
       pie = predictions_error$aR2, # provide errors as input
       pieColor = rep('#5785a2',24),
       #edge.color = obj$pairwise$edgecolor,
       labels = colnames, 
       theme = "TeamFortress", 
       label.color = 'grey10',
       border.color = '#5785a2',
       border.width = 0.5,
       title = "VIA Predictability XGBOOST")
dev.off();
```


```{r}
ggplot(predictions_error, aes(x=1:24))+
  geom_line(aes(y=aR2))+
  geom_point(aes(y=aR2))+
  xlab("") + ylab(TeX("$aR^2$")) + ggtitle("Node Predictability XGBOOST")+ 
  labs(colour="")+
  scale_x_continuous(breaks = 1:24,label = colnames(via_factors[,8:31]))+
  theme_bw()+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
dev.copy(png,
         filename="img/node_predictability_XGBOOST.png", height=2000, 
         width=4000, res = 300);
dev.off();

```

# Stability

Function to get the stability of a network based on the @costenbader_stability_2003 scheme:
[stability_algorithm](stability_algorithm.png)!

To study stability of the networks we followed the work of Costenbader and Valente (2003), we correlated 25 times the original Expected Influence with several subsamples of differents percentages from 1 to 0.5 (half of the sample). We calculated the mean and represent in the figure xxx, in a red area representing the confident interval (.95)

# Computing of Strengh Stability
```{r}
strengh_stability <- function(network,n_samples=25,low_ratio=0.5, 
                              increment = -0.05, 
                              medida_centralidad= 'InExpectedInfluence'){
  
  #computing the strengh stability of a network with Costenbader and Valente
  centrality_cor <- vector() #where the values of centrality stability of each subset
  percentage <- seq(1, low_ratio, by = increment) # percentages of a sample
  steps <- seq(1,n_samples,by=1) #number of times to calculate cor
  a0 <- centrality(estimateNetwork(network, default="EBICglasso", tuning=0))
  # strength of all the df
  number_percentage <- seq(1,length(percentage),by = 1) # vector with the 
  #number of percentages calculated
  df <- setNames(data.frame(matrix(ncol = n_samples, nrow = 0)), c(sprintf("sample_%s",seq(1:n_samples))))
  # empty df for iterations, each row contain a vector
    #with values of differents correlations between a0 and the new sample with % of the original df
  iteration<-vector() #vector where will be saved the values with one percentage of a sample
  pb <- txtProgressBar(min = 1, max = length(number_percentage), style = 3)
  for (i in number_percentage){ #for each value of number_percentage
    setTxtProgressBar(pb, i)
    print()
    pb2 <- txtProgressBar(min = 1, max = length(steps), style = 3)
    for (j in steps){ # and for each step
      setTxtProgressBar(pb2, j)
      a<-centrality(estimateNetwork(sample_frac(network,percentage[i]), default="EBICglasso", tuning=0))
      #compute the strength of a sample_frac
      b<- cor(a0[[medida_centralidad]], a[[medida_centralidad]])
      #and calculate the correlation with a0
      iteration[j]<-b
      #save it in vector iteration
    }
    
    df[i,]<-iteration # and each iteration save it in df
  }
  return(df)
}
```




# Ploting in GGplot
first we can operate with our dataset: (tarda un huevo)
```{r, message=FALSE}
df_stability<-strengh_stability(via_factors[,8:31], low_ratio = 0.05)
```



 


```{r}
row_names<-c("1", ".95", ".90", ".85", ".80", ".75", ".70", ".65", ".60", ".55", ".50",
             ".45", ".40", ".35", ".30", ".25", ".20", ".15", ".10", ".05")
rownames(df_stability)<-row_names
low_ratio <- 0.05
increment <-  -0.05
percentage <- seq(1, low_ratio, by = increment)
number_percentage <- seq(1,length(percentage),by = 1)
df_stability["index"] <- seq(1,length(percentage), by = 1)
df_stability_long <- melt(df_stability, id="index")
df_stability_long["Centrality"] <- "ExpectedInfluence"
tbl_byindex <- as_tibble(group_by(df_stability_long, index))
tbl_summarise <- mutate(tbl_byindex,
                           n = n(),
                           mean = mean(value),
                           max = max(value),
                           min = min(value),
                           median = median(value),
                           sd = sd(value),
                           sem = sd / sqrt(n - 1),
         CI_lower = mean + qt((1-0.95)/2, n - 1) * sem,
         CI_upper = mean - qt((1-0.95)/2, n - 1) * sem)


```


```{r}
ggplot(tbl_summarise, aes(x=index, y=mean)) +
  geom_line(aes(x=index, y=mean, color = Centrality)) +
  geom_ribbon(aes(ymin=min,ymax=max),color="grey90",alpha=0.2) + 
  geom_ribbon(aes(ymin=CI_lower,ymax=CI_upper, fill = Centrality),color="grey70",alpha=0.2) +
  xlab("Percentaje of the sample") + ylab("Correlation") + 
  ggtitle("Correlation of strength of differents sample \n percentages with original dataset") +
  scale_x_continuous(breaks = number_percentage,label = row_names) +
  theme_gray()
dev.copy(png,
         filename="img/stability_centrality.tiff", height=3000, width=5000, res = 300);
dev.off();
```


