---
title: "Preliminary Study of VIA"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
  html_notebook: default
---

# Fisrt steps
packages
```{r, include=FALSE}
library("qgraph")
library("bootnet")
library("dplyr")
library("NetworkComparisonTest")
library("mgm")
library("RColorBrewer")
library("viridis")
library(reshape2) #to melt correlation matrix
library(ggplot2)
library(reshape2) #to reshape objects in R
library(gplots)
library(igraph)
library(devtools)
library(FactoMineR)#install from github @kassambara/factoextra
library(xlsx)
library("FactoMineR") 
library("factoextra")
library("readxl")
library("latex2exp")
library('wTO')
library('EGAnet')
library('progress')
library(psych) 
library("writexl")
```

Conclusiones del 15/06/2021
- Glasso
- Expected Influence
- Predictability XGBOOST
- Communities con Spinglass
  - Bootstraping para calcular (x hacer)
- Bridge Centrality (que todo mantenga la misma estética, en rosita)
- Accuracy and stability con mi método (x hacer)


Creating the dataset to work in R.
```{r}
via_raw<-read.table("Network.dat", 
           header=TRUE, sep="\t")
```



Transforming columns 6 and 7 as.Date:
```{r, include=FALSE}
via_raw$DateOfBirth<-as.Date(via_raw$DateOfBirth,format="%m/%d/%Y")
via_raw$SurveyDate<-as.Date(via_raw$SurveyDate,format="%m/%d/%Y")
```
creating new subset with factors:
```{r}
via_factors<-via_raw[,1:31]
```

```{r}
#haciendo una tabla de estadísticos
stats <- describe(via_factors)
stats$vars <- row.names(stats)
write_xlsx(stats,"stats.xlsx")
```



viewing all the histograms:
```{r}
# Reduzco a 5 dígitos el nombre para que sea más legible en nodos:
colnames <- c("Beaut","Brave","Love","Prude","TWork",     
"Creat","Curio","Fairn","Forgi","Grati", "Hones", "Hope", "Humor", "Perse", "Judgm", "Kindn", "Leade", "Learn", "Humil", "Persp", "SelfR", "SocIQ", "Spiri", "Zest")
```


```{r}
#Histograms
via_hist_df<-via_factors[,8:31]
layout(matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24), 4, 6, byrow = TRUE))
colnames <- colnames
for (i in 1:24) {
    hist(via_hist_df[,i], main=colnames[i], probability=TRUE, col="black", border="white")
}

```



## Regularized Partial correlation network:
Lets try to compute lasso regularized gaussian graphical model and Triangulated Maximally Filtered Graph. La idea subyacente del TMFG consiste en construir una triangulación que maximice una función de puntuación asociada a la cantidad de información retenida por el grafo. El TMFG utiliza como pesos cualquier medida de similitud arbitraria para ordenar los datos en una estructura de red significativa que puede utilizarse para la agrupación, la detección de comunidades y el modelado.

```{r}
set.seed(666)
via_GGM <- estimateNetwork(via_factors[,8:31], default="EBICglasso", tuning=0.4)

```
visualizing:
```{r fig.height=10}
pdf("img/network_glasso.pdf", width = 20, height = 15)
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="grey50",legend=FALSE, aspect=T, labels = colnames, theme = "TeamFortress", label.color = 'grey10', border.color = 'grey50' )
dev.off()
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="grey50",legend=FALSE, aspect=T, labels = colnames, theme = "TeamFortress", label.color = 'grey10', border.color = 'grey50')

```





We can create a object of adjacency Matrix:
```{r}
via_adjacencyMatrix <- via_GGM$graph
```

## Heat plot:

```{r}
heatmap.2(via_adjacencyMatrix, 
        main="Adjacency Matrix", 
        trace="none",
        col = viridis(24),
        scale="none")

```

# Centrality analysis
## EBIC GLASSO

Node strength
```{r}
centRes <- centrality(via_GGM)

# Node strength (degree):
centRes$OutDegree # Or InDegree, it's the same en networks sin dirección
```
```{r}
# Closeness:
centRes$Closeness
```

```{r}
# Betweenness:
centRes$Betweenness
```
ploting:
```{r}
centralityPlot(via_GGM, include = 'all')
```





```{r}
pdf("img/expectedInfluence.pdf", width = 10, height = 5)
centralityPlot(GGM =  via_GGM, include = 'ExpectedInfluence', scale = 'relative', orderBy='ExpectedInfluence')
dev.off()
centralityPlot(GGM =  via_GGM, include = 'ExpectedInfluence', scale = 'relative', orderBy='ExpectedInfluence')
```






# Community detection

M. Girvan y M. E. J. Newman en una de sus investigaciones, han destacado la estructura-propiedad de la comunidad utilizando redes sociales y redes biológicas. Según ellos, los nodos de la red están estrechamente conectados en grupos de punto dentro de las comunidades y débilmente conectados entre las comunidades.

Por qué comunidades y no clustering (aprendizaje no supervisado)
Se puede argumentar que la detección de comunidades es similar al clustering. El clustering es una técnica de aprendizaje automático en la que puntos de datos similares se agrupan en el mismo cluster basándose en sus atributos. Aunque el clustering puede aplicarse a las redes, es un campo más amplio del aprendizaje automático no supervisado que se ocupa de múltiples tipos de atributos. Por otro lado, la detección de comunidades está especialmente diseñada para el análisis de redes, que depende de un único tipo de atributo: edges. Además, los algoritmos de clustering tienden a separar los nodos periféricos únicos de las comunidades a las que deberían pertenecer. Sin embargo, tanto las técnicas de clustering como las de detección de comunidades pueden aplicarse a muchos problemas de análisis de redes y pueden plantear diferentes pros y contras en función del ámbito.
Los métodos de detección de comunidades pueden clasificarse a grandes rasgos en dos tipos: métodos aglomerativos y métodos divisivos. En los métodos aglomerativos, las aristas se añaden una a una a un gráfico que sólo contiene nodos. Las aristas se añaden de la más fuerte a la más débil. Los métodos divisivos son lo contrario a los métodos aglomerativos. En ellos, las aristas se eliminan una a una de un gráfico completo.

## Igraph
We can create a net from adjacency matrix
```{r fig.height=10}
via_net<-graph_from_adjacency_matrix(via_adjacencyMatrix, mode="undirected")
```
We can try with 
```{r}
 
# Estimate networks, first a Gaussian Graphical Model, then an Information Filtering Network:
data = via_factors[,8:31]



set.seed(2018)
graphGGM<-plot(via_GGM)

g = as.igraph(graphGGM, attributes=TRUE)




```




## GALSSO



Community Strucure Via Short Random Walks
This function tries to find densely connected subgraphs, also called communities in a graph via random walks. The idea is that short random walks tend to stay in the same community.

```{r}
lc <- walktrap.community(g)
mem <- membership(lc)
com <- communities(lc)
pdf("img/walktrap.pdf", width = 10, height = 5)
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="black", legend=F, aspect=T, groups = com, title="Comunities in VIA (Community Strucure Via walktrap.community) GLASSO")
dev.off()
plot(via_GGM_plot)

```

```{r}
modularity(g,mem, weights = NULL)
```

Spinglass algorithm


```{r}
lc <- spinglass.community(g, weights = )
mem <- membership(lc)
com <- communities(lc)
pdf("img/spinglass.pdf", width = 10, height = 5)
via_GGM_plot<-plot(via_GGM, layout="spring", vsize=6, cut=0, border.width=1.5, border.color="black", legend=F, aspect=T, groups = com, title="Comunities in VIA (Community Strucure Via SPINGLASS) GLASSO")
dev.off()
plot(via_GGM_plot)

```
```{r}
modularity(g,mem, weights = abs(E(g)$weight))
```

# Probabilidad de que el nodo i esté en comunidad con el nodo j
El algoritmo tiene varias partes:
1. Coge una muestra de la base de datos
2. Hace una red por correlaciones parciales Glasso
3. Calcula comunidades (Spinglass)
4. Extrae las comunidades
5. Para cada nodo i calcula la probabilidad de estar con cada nodo i+1 ... i+n-1 (siendo n=24 el número de nodos totales en el caso del VIA). Notar que en cada iteración la matriz será dicotómica, de 0 y 1
6. Se declara una matrix con los resultados para la primera iteración (es matriz binaria)
7. Se suma con la matrix general, que comienza siendo una matriz de ceros y en cada iteración suma el nuevo resultado
8. Una vez terminado todo el loop, se divide la matriz por el número de bootstrapping que se hagan
9. Se grafican resultados

```{r}
iter = 100 #número de iteraciones totales del bootstrapping
n_sample = 10000 #numero de observaciones por sampleo
megamatrix <- via_adjacencyMatrix * 0 #inicio una megamatrix con todas las probabilidades a 0
magamatrix_zero <- via_adjacencyMatrix * 0 # otra que me sirve para poner a cero las samples
# spinglass
for(i in 1:iter) {
# Tomando una muestra de la base de datos
df_sample = sample_n(via_factors, n_sample)
#Hace una red por correlaciones parciales Glasso
via_GGM_sample <- estimateNetwork(df_sample[,8:31], default="EBICglasso", tuning=0)
#Calcula comunidades (Spinglass)
pdf(paste("img/com/redes/red_bs", "_",i,".pdf"), width = 10, height = 5)
graphGGM_sample <- plot(via_GGM_sample)
dev.off()
g_sample = as.igraph(graphGGM_sample)
sg_sample <- walktrap.community(g_sample)
#Extraigo comunidades
com_sample <- communities(sg_sample)
#ploteo solo para tener en cuenta lo que va pasando
pdf(paste("img/com/communities/spinglass_bs", "_",i,".pdf"), width = 10, height = 5)
via_GGM_plot_sample<-plot(via_GGM_sample, layout="spring", vsize=6, cut=0, 
                          border.width=1.5, border.color="black", 
                          legend=F, aspect=T, 
                          groups = com_sample, 
                          title="Comunities in VIA (Community Strucure Via SPINGLASS) GLASSO")
dev.off()
megamatrix_sample <- magamatrix_zero
n_grupos <- length(com_sample)
for(nodo in 1:nrow(megamatrix)){
  for(grupo in 1:n_grupos){
    if (nodo %in% com_sample[[grupo]]){
      megamatrix_sample[nodo, com_sample[[grupo]]] <-1
    }
  }
  
}
megamatrix <- megamatrix + megamatrix_sample #sumo el resultado a la megamatrix
#por cada iteración del bootstraping
}



megamatrix <- megamatrix/(iter)

```

```{r}
pdf("img/comunities_probabilities.pdf", width = 20, height = 15)
graph_BS<-qgraph(megamatrix,  layout="spring", vsize=6, cut=0, border.width=1.5, legend=F, aspect=T, groups = com, title="Comunities probabilities bootstraping in VIA (Community Strucure Via SPINGLASS) GLASSO", labels = colnames, theme = "TeamFortress",  curveAll=0.5)
dev.off()
```
```{r}
pdf("img/expectedInfluence_bootstraping.pdf", width = 10, height = 5)
centralityPlot(GGM =  megamatrix, include = 'all', scale = 'relative')
dev.off()
centralityPlot(GGM =  megamatrix, include = 'all', scale = 'relative')
```


# Bridge Centrality
Tiene en cuenta las comunidades, por lo tanto, deberíamos de decidir qué comunidades elegimos y con qué método (ver después). Una vez decididas las comunidades, se puede meter como argumento en bridge centrality para estudiar la centralidad entre comunidades de los nodos con el paquete de Jones. Por ahora lo desarrollo según el cálculo por defecto de las comunidades que aparece en el paquete, que según su propio código "if communities not supplied, use spinglass default settings to detect" utiliza Spinglass.


```{r}
# hago las redes
require(qgraph)
n = nrow(via_factors)
network = via_GGM_plot
require("networktools")
bridge_centrality <- bridge(network)
bridge_centrality
```

```{r}
# ploteamos:
pdf("img/bridge_centrality_glasso.pdf")
plot(bridge_centrality, include=c("Bridge Strength", "Bridge Expected Influence (1-step)"), zscore=FALSE)
dev.off()
plot(bridge_centrality, include=c("Bridge Strength", "Bridge Expected Influence (1-step)"), zscore=FALSE)
```


Podemos seleccionar los que estén en el 80% del Exp-influence:

```{r}
BEI1 <- bridge_centrality$`Bridge Expected Influence (1-step)`
communities <- bridge_centrality$communities
top_bridges <- names(BEI1[BEI1>quantile(BEI1, probs=0.80, na.rm=TRUE)])
top_bridges
```

Generamos un nuevo grupo en nuestro communities que sean los más bridge EI


```{r}
bridge_num_w1 <- which(names(BEI1) %in% top_bridges)
new_communities <- vector()
for(i in 1:length(BEI1)) {
  if(i %in% bridge_num_w1) {
    new_communities[i] <- "Bridge"
  } else {new_communities[i] <- communities[i]}
}
```

```{r}
pdf("img/communities_new_group.pdf")
grafo <-qgraph(network, layout="spring", 
       groups=new_communities, color=c("red","#eee361", "#6fb3e4", "#469b77", "#d16d6f", "#bbb2d4"), 
       legend.cex = 0.4)
dev.off()
qgraph(network, layout="spring", 
       groups=new_communities, color=c("red","#eee361", "#6fb3e4", "#469b77", "#d16d6f", "#bbb2d4"), 
       legend.cex = 0.4)
```



# Computing Predictability 
we would like to know: How accurate can the prediction be for a given node knowing all the others? Predictability is interesting for several reasons: 1. Even if a node has several edges but you can only predict 1% of its variance, then, we could come to the conclusion that we cannot assure that by influencing the nodes to which it is connected you can have good results.
2. It tells us to what extent the different parts of the network are self-determined or determined by other factors that are not included in the network.

Ahora se puede sacar la predictabilidad que tenemso de utilizar otros métodos de machine learning. Véase el jupyter notebook.

```{r}
#### Gráfico 1. Práctica - Blink
predictions_error <- read_xlsx("predictions_error.xlsx")[,2:25]

predictions_error <- t(predictions_error)
colnames(predictions_error) <- c("RMSE","MAE", "R2","aR2")
predictions_error <- as.data.frame(predictions_error)
predictions_error[predictions_error<=0] <- 0
```

```{r}

pdf("img/predictability.pdf", width = 20, height = 15)
qgraph(via_GGM$graph, # weighted adjacency matrix as input
       cut=0,
       legend=FALSE, aspect=T,
       layout = 'spring', 
       pie = predictions_error$aR2, # provide errors as input
       pieColor = rep('#5785a2',24),
       #edge.color = obj$pairwise$edgecolor,
       labels = colnames, 
       theme = "TeamFortress", 
       label.color = 'grey10',
       border.color = '#5785a2',
       border.width = 0.5,
       title = "VIA Predictability XGBOOST")
dev.off();
```


```{r}
ggplot(predictions_error, aes(x=1:24))+
  geom_line(aes(y=aR2))+
  geom_point(aes(y=aR2))+
  xlab("") + ylab(TeX("$aR^2$")) + ggtitle("Node Predictability XGBOOST")+ 
  labs(colour="")+
  scale_x_continuous(breaks = 1:24,label = colnames(via_factors[,8:31]))+
  theme_bw()+ theme(axis.text.x = element_text(angle = 45, hjust = 1))
dev.copy(png,
         filename="img/node_predictability_XGBOOST.png", height=2000, 
         width=4000, res = 300);
dev.off();

```

# Estabilidad

Función para sacar la estabilidad de una red basado en el esquema de @costenbader_stability_2003:
[Algoritmo de estabilidad](algoritmo_estabilidad.png)!

To study stability of the networks we followed the work of Costenbader and Valente (2003), we correlated 25 times the original Expected Influence with several subsamples of differents percentages from 1 to 0.5 (half of the sample). We calculated the mean and represent in the figure xxx, in a red area representing the confident interval (.95)

# Computing of Strengh Stability
```{r}
strengh_stability <- function(network,n_samples=25,low_ratio=0.5, 
                              increment = -0.05, 
                              medida_centralidad= 'InExpectedInfluence'){
  
  #computing the strengh stability of a network with Costenbader and Valente
  centrality_cor <- vector() #where the values of centrality stability of each subset
  percentage <- seq(1, low_ratio, by = increment) # percentages of a sample
  steps <- seq(1,n_samples,by=1) #number of times to calculate cor
  a0 <- centrality(estimateNetwork(network, default="EBICglasso", tuning=0))
  # strength of all the df
  number_percentage <- seq(1,length(percentage),by = 1) # vector with the 
  #number of percentages calculated
  df <- setNames(data.frame(matrix(ncol = n_samples, nrow = 0)), c(sprintf("sample_%s",seq(1:n_samples))))
  # empty df for iterations, each row contain a vector
    #with values of differents correlations between a0 and the new sample with % of the original df
  iteration<-vector() #vector where will be saved the values with one percentage of a sample
  pb <- txtProgressBar(min = 1, max = length(number_percentage), style = 3)
  for (i in number_percentage){ #for each value of number_percentage
    setTxtProgressBar(pb, i)
    print()
    pb2 <- txtProgressBar(min = 1, max = length(steps), style = 3)
    for (j in steps){ # and for each step
      setTxtProgressBar(pb2, j)
      a<-centrality(estimateNetwork(sample_frac(network,percentage[i]), default="EBICglasso", tuning=0))
      #compute the strength of a sample_frac
      b<- cor(a0[[medida_centralidad]], a[[medida_centralidad]])
      #and calculate the correlation with a0
      iteration[j]<-b
      #save it in vector iteration
    }
    
    df[i,]<-iteration # and each iteration save it in df
  }
  return(df)
}
```




# Ploting in GGplot
first we can operate with our dataset: (tarda un huevo)
```{r, message=FALSE}
df_stability<-strengh_stability(via_factors[,8:31], low_ratio = 0.05)
```



 

Lo siguiente es puta magia con dplyr, se puede ver el [siguiente tutorial](http://thenode.biologists.com/visualizing-data-one-more-time/education/)

```{r}
row_names<-c("1", ".95", ".90", ".85", ".80", ".75", ".70", ".65", ".60", ".55", ".50",
             ".45", ".40", ".35", ".30", ".25", ".20", ".15", ".10", ".05")
rownames(df_stability)<-row_names
low_ratio <- 0.05
increment <-  -0.05
percentage <- seq(1, low_ratio, by = increment)
number_percentage <- seq(1,length(percentage),by = 1)
df_stability["index"] <- seq(1,length(percentage), by = 1)
df_stability_long <- melt(df_stability, id="index")
df_stability_long["Centrality"] <- "ExpectedInfluence"
tbl_byindex <- as_tibble(group_by(df_stability_long, index))
tbl_summarise <- mutate(tbl_byindex,
                           n = n(),
                           mean = mean(value),
                           max = max(value),
                           min = min(value),
                           median = median(value),
                           sd = sd(value),
                           sem = sd / sqrt(n - 1),
         CI_lower = mean + qt((1-0.95)/2, n - 1) * sem,
         CI_upper = mean - qt((1-0.95)/2, n - 1) * sem)


```


```{r}
ggplot(tbl_summarise, aes(x=index, y=mean)) +
  geom_line(aes(x=index, y=mean, color = Centrality)) +
  geom_ribbon(aes(ymin=min,ymax=max),color="grey90",alpha=0.2) + 
  geom_ribbon(aes(ymin=CI_lower,ymax=CI_upper, fill = Centrality),color="grey70",alpha=0.2) +
  xlab("Percentaje of the sample") + ylab("Correlation") + 
  ggtitle("Correlation of strength of differents sample \n percentages with original dataset") +
  scale_x_continuous(breaks = number_percentage,label = row_names) +
  theme_gray()
dev.copy(png,
         filename="img/stability_centrality.tiff", height=3000, width=5000, res = 300);
dev.off();
```


